---
title: "Data Acquisition"
author: "Lina Meyer"

---
# Weather

```{r, results='asis'}
#| echo: false
#| message: false
#| warning: false

library(httr)
library(glue)
library(jsonlite)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggdark)


api <- function(url = "") {
  resp <- GET(url)
  stop_for_status(resp) # automatically throws an error if a request did not succeed
}


weather_imgs <- api("https://gist.githubusercontent.com/stellasphere/9490c195ed2b53c707087c8c2db4ec0c/raw/db92e194f4f2109a68a706e46bc624eb3cbe3889/descriptions.json") %>%
                .$content %>% 
                rawToChar() %>% 
                fromJSON()

weather_json <- api("https://api.open-meteo.com/v1/dwd-icon?latitude=53.55&longitude=9.99&hourly=temperature_2m,relativehumidity_2m,apparent_temperature,rain,weathercode,cloudcover,windspeed_10m,winddirection_10m&timezone=Europe%2FBerlin") %>% 
                .$content %>% 
                rawToChar() %>% 
                fromJSON()

current_time <- format(Sys.time(), "%Y-%m-%dT%H:00")

curr_idx <- which(sapply(weather_json[["hourly"]][["time"]], function(y) current_time %in% y))

curr_temp <- weather_json[["hourly"]][["temperature_2m"]][[curr_idx]]
curr_hum <- weather_json[["hourly"]][["relativehumidity_2m"]][[curr_idx]]
curr_apparent_temp <- weather_json[["hourly"]][["apparent_temperature"]][[curr_idx]]
curr_rain <- weather_json[["hourly"]][["rain"]][[curr_idx]]
curr_wcode <- weather_json[["hourly"]][["weathercode"]][[curr_idx]]
curr_wind <- weather_json[["hourly"]][["windspeed_10m"]][[curr_idx]]
curr_winddir <- weather_json[["hourly"]][["winddirection_10m"]][[curr_idx]]

curr_weather_img <- weather_imgs[[as.character(curr_wcode)]][["day"]][["image"]]
curr_weather_desc <- weather_imgs[[as.character(curr_wcode)]][["day"]][["description"]]

current_time_nice_format <- format(Sys.time(), "the %d.%m.%y at %H:%M")
cat(glue("On {current_time_nice_format} it was {curr_weather_desc} in Hamburg"))

render_weather_img <- glue("[![]({curr_weather_img})](http://openweathermap.org)")

cat(render_weather_img)

curr_weather_tab <- glue("|                      |                         |  
                          |:---------------------|------------------------:|
                          | temperature          | {curr_temp} °C          |  
                          | apparent temperature | {curr_apparent_temp} °C |
                          | relative humidity    | {curr_hum} %            |  
                          | rain                 | {curr_rain} mm          |
                          | wind speed           | {curr_wind} km/h        |
                          | wind direction       | {curr_winddir} °        |
                          ")

  
cat(curr_weather_tab)

```

Temperature Forecast for the next week was:
```{r plot, dev.args = list(bg = 'transparent')}
#| echo: false
#| message: false
#| warning: false

weather_tbl <- as_tibble(weather_json[["hourly"]]) %>%
               mutate(time = as.POSIXct(time, format = "%Y-%m-%dT%H:00"))

max_temp <- weather_tbl %>% 
            summarize(max(temperature_2m)) %>%
            first %>% first %>%
            as.numeric()

min_temp <- weather_tbl %>% 
            summarize(min(temperature_2m)) %>%
            first %>% first %>%
            as.numeric()

weather_tbl %>%
  
  # Set up x, y
  ggplot(aes(x = time, y = temperature_2m, group=1)) +
  
  geom_line() + 
  dark_theme_gray() +
  theme(#panel.background = element_rect(fill='transparent'),
        plot.background = element_rect(fill='transparent', color=NA)) +
  scale_x_datetime(name="", breaks = NULL, date_breaks = "1 day", date_labels = "%d/%m")+
  scale_y_continuous(name="°C", limits=c(min_temp, max_temp))
```

# Radon Bikes 

```{r}
#| echo: false
#| message: false
#| warning: false

# WEBSCRAPING ----

# 1.0 LIBRARIES ----

library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing


# 1.1 COLLECT PRODUCT FAMILIES ----

url_home          <- "https://www.radon-bikes.de/"

# Read in the HTML for the entire webpage
html_home         <- read_html(url_home)

# Web scrape the ids for the families
bike_category_tbl <- html_home %>%
  
  # Get the nodes for the families ...
  html_nodes(css = ".megamenu__item > a") %>%
  html_attr('href') %>%
    
  # Remove the product families Gear and Outlet and Woman 
  # (because the female bikes are also listed with the others)
  discard(.p = ~stringr::str_detect(.x,"wear")) %>%
  
  # Convert vector to tibble
  enframe(name = "position", value = "subdirectory") %>%
  
  # Add the domain, because we will get only the subdirectories
  mutate(
    url = glue("https://www.radon-bikes.de{subdirectory}bikegrid/")
  )


# 2.0 COLLECT BIKE DATA ----
get_bike_data <- function(bike_category_url) {
  
  # get category name
  category_name = bike_category_url %>% str_extract("(?<=/)[:alpha:]+(?=/bikegrid/)")
  
  # Wait between each request to reduce the load on the server 
  # Otherwise we could get blocked
  Sys.sleep(3)
  # Get the names, prices and URLs for the bikes of the first category
  html_bike_category  <- read_html(bike_category_url)
  
  bike_model_names_tbl <- html_bike_category %>%
    html_nodes(css = ".m-bikegrid__info > a > div > .a-heading.a-heading--small")%>%
    html_text()%>%
    str_extract("[:alnum:].*[:alnum:]")%>%
    # Convert vector to tibble
    enframe(name = "position", value = "model_name")
  
  bike_model_prices <- html_bike_category %>%
    html_nodes(css = ".m-bikegrid__info > a > div > .m-bikegrid__price.currency_eur > .m-bikegrid__price--active")%>%
    html_text()%>%
    str_extract("[0-9]+")%>%
    # Convert vector to tibble
    enframe(name = "position", value = "model_price") %>%
    mutate(model_price = as.numeric(model_price)) %>%
    mutate(model_price = scales::dollar(model_price, big.mark = ".", 
                                        decimal.mark = ",", 
                                        prefix = "", 
                                        suffix = " €"))
  
  bike_model_urls <- html_bike_category %>%
    html_nodes(css = ".m-bikegrid__info > a")%>%
    html_attr("href")%>%
    # Convert vector to tibble
    enframe(name = "position", value = "model_url")%>%
    # Add the domain, because we will get only the subdirectories
    mutate(
      model_url = glue("https://www.radon-bikes.de{model_url}")
    )
  
  
  bikes_tbl <- left_join(bike_model_names_tbl, bike_model_prices) %>%
    left_join(bike_model_urls) %>%
    add_column(category_name)
  return(bikes_tbl)
}

# Run the function with the first url to check if it is working
bike_category_url <- bike_category_tbl$url[1]
bike_data_tbl     <- get_bike_data(bike_category_url)


# 2.3.1a Map the function against all urls

# Extract the urls as a character vector
bike_category_url_vec <- bike_category_tbl %>% 
  pull(url)

# Run the function with every url as an argument
bike_data_lst <- map(bike_category_url_vec, get_bike_data)

# Merge the list into a tibble
bike_data_tbl <- bind_rows(bike_data_lst)

bike_data_tbl
```
